import os
import random
import requests
import re
from dotenv import load_dotenv
from django.core.management.base import BaseCommand
from books.models import Book, Author, Category

load_dotenv()

class Command(BaseCommand):
    help = 'ë„ì„œ ìˆ˜ì§‘ ë° ì‘ê°€ ì •ë³´ë¥¼ ìœ„í‚¤í”¼ë””ì•„ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤ (ì •ì œëœ ì´ë¦„ ê¸°ì¤€)'

    def handle(self, *args, **options):
        ttb_key = os.getenv("ALADIN_TTBKEY")
        if not ttb_key:
            print("âŒ ALADIN_TTBKEY í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        else:
            print(f"âœ… ALADIN_TTBKEY ë¡œë“œ ì„±ê³µ: {ttb_key[:5]}******")

        keywords = [
            'ì‚¬ë‘', 'ìš°ì •', 'ì„±ì¥', 'ì² í•™', 'ê°ë™', 'ì¶”ë¦¬', 'ìê¸°ê³„ë°œ', 'ì¸ë¬¸', 'ì—ì„¸ì´', 'ì—¬í–‰',
            'ì²­ì¶˜', 'ê°€ì¡±', 'ì—­ì‚¬', 'í™˜ê²½', 'ì „ìŸ', 'ì¸ìƒ', 'ì‚¶', 'ì£½ìŒ', 'ê³ ì „', 'ê³¼í•™',
            'SF', 'íŒíƒ€ì§€', 'ì˜ˆìˆ ', 'ì‹œ', 'ìˆ˜í•„', 'ë¬¸í™”', 'êµìœ¡', 'ì‹¬ë¦¬', 'ì§ì¥', 'ë¦¬ë”ì‹­',
            'ë¹„ì¦ˆë‹ˆìŠ¤', 'ì°½ì—…', 'ê¸°ìˆ ', 'AI', 'ë¹…ë°ì´í„°', 'ì—°ì• ', 'ì¤‘ë…„', 'ë¯¸ë˜', 'ì‚¬íšŒ', 'ì •ì¹˜'
        ]
        random.shuffle(keywords)

        collected_isbns = set()
        total_saved = 0
        max_books = 500

        total_saved = self.fetch_bestsellers(ttb_key, collected_isbns, total_saved, max_books)

        for keyword in keywords:
            if total_saved >= max_books:
                break

            print(f"\nğŸ” í‚¤ì›Œë“œ '{keyword}'ì—ì„œ ì±… ìˆ˜ì§‘ ì¤‘...")

            search_url = "https://www.aladin.co.kr/ttb/api/ItemSearch.aspx"
            search_params = {
                "ttbkey": ttb_key,
                "Query": keyword,
                "QueryType": "Keyword",
                "MaxResults": 100,
                "start": 1,
                "SearchTarget": "Book",
                "output": "js",
                "Version": "20131101",
                "Cover": "Big"
            }

            try:
                response = requests.get(search_url, params=search_params)
                print(f"[ì‘ë‹µ ìƒíƒœ ì½”ë“œ] {response.status_code}")

                if response.status_code != 200:
                    print(f"âš ï¸ API ì˜¤ë¥˜ ì‘ë‹µ:\n{response.text[:300]}...")
                    continue

                data = response.json().get('item', [])
                print(f"ğŸ“š '{keyword}' í‚¤ì›Œë“œë¡œ {len(data)}ê¶Œ ìˆ˜ì‹ ")

            except requests.RequestException as e:
                print(f"ğŸ”´ ìš”ì²­ ì˜ˆì™¸ ë°œìƒ: {e}")
                continue

            for item in data:
                if total_saved >= max_books:
                    break
                total_saved = self.save_book_from_item(item, collected_isbns, total_saved)

        print(f"\nğŸ“¦ ì´ ì €ì¥ ì™„ë£Œ: {total_saved}ê¶Œ")

    def fetch_bestsellers(self, ttb_key, collected_isbns, total_saved, max_books):
        print("\nğŸ”¥ [ë² ìŠ¤íŠ¸ì…€ëŸ¬ ìˆ˜ì§‘ ì‹œì‘]")
        url = "https://www.aladin.co.kr/ttb/api/ItemList.aspx"
        params = {
            "ttbkey": ttb_key,
            "QueryType": "Bestseller",
            "MaxResults": 50,
            "start": 1,
            "SearchTarget": "Book",
            "output": "js",
            "Version": "20131101",
            "Cover": "Big"
        }

        try:
            response = requests.get(url, params=params)
            if response.status_code != 200:
                print(f"âŒ ë² ìŠ¤íŠ¸ì…€ëŸ¬ API ì˜¤ë¥˜:\n{response.text}")
                return total_saved

            data = response.json().get('item', [])
            print(f"ğŸ“š ë² ìŠ¤íŠ¸ì…€ëŸ¬ {len(data)}ê¶Œ ìˆ˜ì‹ ")

            for item in data:
                if total_saved >= max_books:
                    break
                total_saved = self.save_book_from_item(item, collected_isbns, total_saved)

        except Exception as e:
            print(f"ğŸ”» ë² ìŠ¤íŠ¸ì…€ëŸ¬ ìš”ì²­ ì‹¤íŒ¨: {e}")

        return total_saved

    def save_book_from_item(self, item, collected_isbns, total_saved):
        isbn13 = item.get('isbn13')
        if not isbn13 or isbn13 in collected_isbns:
            return total_saved

        title = item.get('title', '').strip()
        raw_author_name = item.get('author', '').strip()
        category_name = item.get('categoryName', '').strip()

        if not title or not raw_author_name:
            return total_saved

        print(f"âœ… ì €ì¥ ëŒ€ìƒ: {title[:30]} / {raw_author_name}")

        author, _ = Author.objects.get_or_create(name=raw_author_name)
        parsed_author_name = self.extract_primary_author_name(raw_author_name)
        wiki_data = self.get_wikipedia_data(parsed_author_name)

        if not wiki_data:
            print(f"ğŸš« ì‘ê°€ '{parsed_author_name}' ìœ„í‚¤ ì •ë³´ ì—†ìŒ â†’ ì €ì¥ ê±´ë„ˆëœ€")
            return total_saved

        author.profile_image = wiki_data.get('thumbnail', {}).get('source')
        author.biography = wiki_data.get('extract', '')
        author.save()

        category, _ = Category.objects.get_or_create(name=category_name)

        Book.objects.update_or_create(
            isbn13=isbn13,
            defaults={
                "title": title,
                "author": author,
                "category": category,
                "description": item.get('description', ''),
                "cover_image_url": item.get('cover', ''),
                "pub_date": item.get('pubDate', ''),
                "publisher": item.get('publisher', ''),
                "link": item.get('link', ''),
                "is_bestseller": bool(item.get('bestRank', 0)),
            }
        )

        collected_isbns.add(isbn13)
        return total_saved + 1

    def extract_primary_author_name(self, raw_author_str):
        parts = [part.strip() for part in raw_author_str.split(',')]
        first = parts[0]
        return re.sub(r'\([^)]*\)', '', first).strip()

    def get_wikipedia_data(self, author_name):
        try:
            titles = self.search_wikipedia_titles(author_name)
            best_title = self.filter_likely_author_title(titles)
            if not best_title:
                print(f"âŒ '{author_name}'ì— ëŒ€í•œ ì ì ˆí•œ ë¬¸ì„œ ì—†ìŒ")
                return None

            summary_url = f"https://ko.wikipedia.org/api/rest_v1/page/summary/{best_title}"
            response = requests.get(summary_url)
            if response.status_code == 200:
                return response.json()
        except Exception as e:
            print(f"ğŸ”» ìœ„í‚¤í”¼ë””ì•„ ìš”ì²­ ì‹¤íŒ¨: {author_name} / {e}")
        return None

    def search_wikipedia_titles(self, author_name):
        try:
            search_url = "https://ko.wikipedia.org/w/api.php"
            params = {
                "action": "opensearch",
                "search": author_name,
                "limit": 5,
                "namespace": 0,
                "format": "json"
            }
            response = requests.get(search_url, params=params)
            if response.status_code == 200:
                _, titles, *_ = response.json()
                print(f"ğŸ” '{author_name}' ê²€ìƒ‰ ê²°ê³¼: {titles}")
                return titles
        except Exception as e:
            print(f"ğŸ” ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

    def filter_likely_author_title(self, titles):
        for title in titles:
            if any(keyword in title for keyword in ['ì‘ê°€', 'ì†Œì„¤ê°€', 'ì‹œì¸', 'ìˆ˜í•„ê°€']):
                return title
        return titles[0] if titles else None
